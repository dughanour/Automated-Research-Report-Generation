# LLM configuration
llm:
  groq:
    provider: "groq"
    model_name: "deepseek-r1-distill-llama-70b"
    temperature: 0
    max_output_tokens: 2048
  
  google:
    provider: "google"
    model_name: "gemini-2.5-flash"
    temperature: 0
    max_output_tokens: 2048
  
  openai:
    provider: "openai"
    model_name: "gpt-4o-mini"
    temperature: 0

# Database configuration
# astra_db:
#   collection_name: "research_reports"

# Embedding configuration
# embedding_model:
#   provider: "google"
#   model_name: "models/text-embedding-004"

# Retrieval configuration
# retriever:
#   top_k: 5
